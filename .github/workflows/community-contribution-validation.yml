name: 🤝 Community Contribution Validation

# Comprehensive validation pipeline for community contributions
# Ensures code quality, security, and compatibility

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    branches: [ main, develop ]
  push:
    branches: [ develop ]
  workflow_dispatch:
    inputs:
      full_validation:
        description: 'Run full validation suite'
        required: false
        default: 'false'
        type: boolean

env:
  NODE_VERSION: '18'
  PYTHON_VERSION: '3.9'
  DOCKER_BUILDKIT: 1
  COMPOSE_DOCKER_CLI_BUILD: 1

jobs:
  # Initial validation and setup
  validation-setup:
    name: 🔍 Validation Setup
    runs-on: ubuntu-latest
    outputs:
      should-run-tests: ${{ steps.changes.outputs.should-run-tests }}
      should-run-integration: ${{ steps.changes.outputs.should-run-integration }}
      should-run-security: ${{ steps.changes.outputs.should-run-security }}
      pr-title: ${{ steps.pr-info.outputs.title }}
      pr-author: ${{ steps.pr-info.outputs.author }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Get PR information
        id: pr-info
        run: |
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            echo "title=${{ github.event.pull_request.title }}" >> $GITHUB_OUTPUT
            echo "author=${{ github.event.pull_request.user.login }}" >> $GITHUB_OUTPUT
          else
            echo "title=Push to ${{ github.ref_name }}" >> $GITHUB_OUTPUT
            echo "author=${{ github.actor }}" >> $GITHUB_OUTPUT
          fi

      - name: Detect changes
        id: changes
        run: |
          # Detect what types of validation to run based on changed files
          if [ "${{ github.event_name }}" = "pull_request" ]; then
            CHANGED_FILES=$(gh api repos/${{ github.repository }}/pulls/${{ github.event.number }}/files | jq -r '.[].filename')
          else
            CHANGED_FILES=$(git diff --name-only HEAD~1)
          fi
          
          echo "Changed files:"
          echo "$CHANGED_FILES"
          
          # Check if we should run various test suites
          should_run_tests="false"
          should_run_integration="false"
          should_run_security="false"
          
          # TypeScript/JavaScript changes
          if echo "$CHANGED_FILES" | grep -E "\.(ts|js|json)$" > /dev/null; then
            should_run_tests="true"
          fi
          
          # Docker or integration files
          if echo "$CHANGED_FILES" | grep -E "(docker|compose|Dockerfile|integration)" > /dev/null; then
            should_run_integration="true"
          fi
          
          # Security-sensitive files
          if echo "$CHANGED_FILES" | grep -E "(package\.json|\.github|security|auth|credential)" > /dev/null; then
            should_run_security="true"
          fi
          
          # Force full validation if requested
          if [ "${{ inputs.full_validation }}" = "true" ]; then
            should_run_tests="true"
            should_run_integration="true"
            should_run_security="true"
          fi
          
          echo "should-run-tests=$should_run_tests" >> $GITHUB_OUTPUT
          echo "should-run-integration=$should_run_integration" >> $GITHUB_OUTPUT
          echo "should-run-security=$should_run_security" >> $GITHUB_OUTPUT
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Welcome new contributors
        if: github.event_name == 'pull_request' && github.event.action == 'opened'
        run: |
          # Check if this is a first-time contributor
          AUTHOR="${{ github.event.pull_request.user.login }}"
          CONTRIB_COUNT=$(gh api "repos/${{ github.repository }}/pulls" --field state=closed --field creator="$AUTHOR" | jq length)
          
          if [ "$CONTRIB_COUNT" -eq 0 ]; then
            echo "Welcome new contributor: $AUTHOR!"
            
            # Post welcome comment (would be implemented with gh api in real scenario)
            echo "This would post a welcome comment for new contributors"
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # Code quality and TypeScript validation
  code-quality:
    name: 📝 Code Quality
    runs-on: ubuntu-latest
    needs: validation-setup
    if: needs.validation-setup.outputs.should-run-tests == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: TypeScript compilation check
        run: |
          echo "Checking TypeScript compilation..."
          npm run validate || (echo "TypeScript compilation failed" && exit 1)

      - name: ESLint check
        run: |
          if [ -f ".eslintrc.js" ] || [ -f ".eslintrc.json" ]; then
            echo "Running ESLint..."
            npx eslint src/ --ext .ts,.js --format=json --output-file=eslint-report.json || true
            
            # Check if there are any errors (not just warnings)
            ERRORS=$(cat eslint-report.json | jq '[.[] | select(.errorCount > 0)] | length')
            if [ "$ERRORS" -gt 0 ]; then
              echo "ESLint found $ERRORS files with errors"
              cat eslint-report.json | jq '.[] | select(.errorCount > 0)'
              exit 1
            fi
          else
            echo "No ESLint configuration found, skipping..."
          fi

      - name: Prettier check
        run: |
          if [ -f ".prettierrc" ] || [ -f "prettier.config.js" ]; then
            echo "Checking code formatting with Prettier..."
            npx prettier --check "src/**/*.{ts,js,json}" || (
              echo "Code formatting issues found. Run 'npx prettier --write .' to fix."
              exit 1
            )
          else
            echo "No Prettier configuration found, skipping..."
          fi

      - name: Upload code quality report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: code-quality-report
          path: |
            eslint-report.json
            typescript-report.json
          retention-days: 30

  # Automated testing
  automated-tests:
    name: 🧪 Automated Tests
    runs-on: ubuntu-latest
    needs: validation-setup
    if: needs.validation-setup.outputs.should-run-tests == 'true'
    strategy:
      matrix:
        test-suite: ['unit', 'integration']
        node-version: ['18', '20']
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js ${{ matrix.node-version }}
        uses: actions/setup-node@v4
        with:
          node-version: ${{ matrix.node-version }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Run unit tests
        if: matrix.test-suite == 'unit'
        run: |
          echo "Running unit tests with coverage..."
          npm test -- --coverage --testPathPattern='.*\.test\.(ts|js)$' --coverageReporters=json-summary,lcov,text
          
          # Check coverage thresholds
          COVERAGE=$(cat coverage/coverage-summary.json | jq '.total.lines.pct')
          echo "Code coverage: $COVERAGE%"
          
          if [ "$(echo "$COVERAGE < 70" | bc)" -eq 1 ]; then
            echo "Warning: Code coverage is below 70%"
          fi

      - name: Run integration tests
        if: matrix.test-suite == 'integration'
        run: |
          echo "Running integration tests..."
          if npm run | grep -q "test:integration"; then
            npm run test:integration
          else
            echo "No integration tests configured"
          fi

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-suite }}-node${{ matrix.node-version }}
          path: |
            coverage/
            test-results.xml
            junit.xml
          retention-days: 30

  # RAG and Docker integration tests
  rag-integration:
    name: 🤖 RAG Integration Tests
    runs-on: ubuntu-latest
    needs: validation-setup
    if: needs.validation-setup.outputs.should-run-integration == 'true'
    services:
      chroma:
        image: ghcr.io/chroma-core/chroma:latest
        ports:
          - 8000:8000
        options: >-
          --health-cmd="curl -f http://localhost:8000/api/v1/heartbeat || exit 1"
          --health-interval=30s
          --health-timeout=10s
          --health-retries=3
      
      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd="redis-cli ping"
          --health-interval=10s
          --health-timeout=5s
          --health-retries=3

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Wait for services
        run: |
          echo "Waiting for Chroma to be ready..."
          for i in {1..30}; do
            if curl -f http://localhost:8000/api/v1/heartbeat; then
              echo "Chroma is ready"
              break
            fi
            sleep 2
          done
          
          echo "Waiting for Redis to be ready..."
          for i in {1..30}; do
            if redis-cli -h localhost ping; then
              echo "Redis is ready"
              break
            fi
            sleep 2
          done

      - name: Run RAG integration tests
        run: |
          echo "Running RAG integration tests..."
          if npm run | grep -q "test:rag"; then
            CHROMA_URL=http://localhost:8000 REDIS_URL=redis://localhost:6379 npm run test:rag
          else
            echo "Creating basic RAG integration test..."
            # This would run a basic RAG connectivity test
            curl -f http://localhost:8000/api/v1/heartbeat
            redis-cli -h localhost ping
            echo "RAG services are accessible"
          fi

      - name: Test Docker development environment
        run: |
          echo "Testing Docker development environment..."
          if [ -f "dev/environment/docker-compose.dev.yml" ]; then
            # Test Docker compose file validity
            docker-compose -f dev/environment/docker-compose.dev.yml config
            echo "Docker compose configuration is valid"
          fi

  # Security and dependency audit
  security-audit:
    name: 🔒 Security Audit
    runs-on: ubuntu-latest
    needs: validation-setup
    if: needs.validation-setup.outputs.should-run-security == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Run npm audit
        run: |
          echo "Running npm security audit..."
          npm audit --audit-level moderate --json > audit-results.json || true
          
          # Check for high/critical vulnerabilities
          HIGH_VULNS=$(cat audit-results.json | jq '.metadata.vulnerabilities.high // 0')
          CRITICAL_VULNS=$(cat audit-results.json | jq '.metadata.vulnerabilities.critical // 0')
          
          echo "High vulnerabilities: $HIGH_VULNS"
          echo "Critical vulnerabilities: $CRITICAL_VULNS"
          
          if [ "$CRITICAL_VULNS" -gt 0 ]; then
            echo "Critical security vulnerabilities found!"
            cat audit-results.json | jq '.advisories[] | select(.severity == "critical")'
            exit 1
          fi
          
          if [ "$HIGH_VULNS" -gt 0 ]; then
            echo "High security vulnerabilities found - review required"
            cat audit-results.json | jq '.advisories[] | select(.severity == "high")'
          fi

      - name: Check for secrets in code
        run: |
          echo "Scanning for potential secrets..."
          
          # Simple secret pattern detection
          SECRET_PATTERNS=(
            "password.*=.*['\"][^'\"]{8,}"
            "api[_-]?key.*=.*['\"][^'\"]{20,}"
            "secret.*=.*['\"][^'\"]{10,}"
            "token.*=.*['\"][^'\"]{20,}"
          )
          
          SECRETS_FOUND=false
          for pattern in "${SECRET_PATTERNS[@]}"; do
            if grep -r -i -E "$pattern" src/ 2>/dev/null; then
              echo "Potential secret found matching pattern: $pattern"
              SECRETS_FOUND=true
            fi
          done
          
          if [ "$SECRETS_FOUND" = true ]; then
            echo "Warning: Potential secrets detected in code"
            echo "Please review and use environment variables or secure storage"
          else
            echo "No obvious secrets detected"
          fi

      - name: Dependency license check
        run: |
          echo "Checking dependency licenses..."
          # This would check for incompatible licenses
          npm ls --json > dependencies.json
          echo "Dependencies analyzed - manual review may be required"

      - name: Upload security report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: security-audit-report
          path: |
            audit-results.json
            dependencies.json
          retention-days: 30

  # Performance and benchmarking
  performance-benchmark:
    name: ⚡ Performance Benchmark
    runs-on: ubuntu-latest
    needs: validation-setup
    if: needs.validation-setup.outputs.should-run-tests == 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Run performance benchmarks
        run: |
          echo "Running performance benchmarks..."
          
          # Check if performance tests exist
          if npm run | grep -q "test:performance"; then
            npm run test:performance
          else
            echo "Creating basic performance benchmark..."
            
            # Basic bundle size check
            BUNDLE_SIZE=$(du -sh dist/ | cut -f1)
            echo "Bundle size: $BUNDLE_SIZE"
            
            # Basic memory usage test
            node -e "
              const start = process.memoryUsage();
              require('./dist/index.js');
              const end = process.memoryUsage();
              console.log('Memory usage delta:', {
                rss: end.rss - start.rss,
                heapUsed: end.heapUsed - start.heapUsed
              });
            "
          fi

      - name: Bundle size analysis
        run: |
          echo "Analyzing bundle size..."
          
          if [ -d "dist" ]; then
            find dist -name "*.js" -type f -exec wc -c {} + | sort -n
            
            TOTAL_SIZE=$(du -sb dist/ | cut -f1)
            echo "Total bundle size: $TOTAL_SIZE bytes"
            
            # Warn if bundle is too large (>5MB)
            if [ "$TOTAL_SIZE" -gt 5242880 ]; then
              echo "Warning: Bundle size is larger than 5MB"
            fi
          fi

  # Contribution validation summary
  validation-summary:
    name: 📊 Validation Summary
    runs-on: ubuntu-latest
    needs: [validation-setup, code-quality, automated-tests, rag-integration, security-audit, performance-benchmark]
    if: always()
    steps:
      - name: Generate validation summary
        run: |
          echo "# Contribution Validation Summary" > validation-summary.md
          echo "" >> validation-summary.md
          echo "**Author:** ${{ needs.validation-setup.outputs.pr-author }}" >> validation-summary.md
          echo "**Title:** ${{ needs.validation-setup.outputs.pr-title }}" >> validation-summary.md
          echo "**Validation Date:** $(date)" >> validation-summary.md
          echo "" >> validation-summary.md
          
          # Check job statuses
          CODE_QUALITY="${{ needs.code-quality.result }}"
          TESTS="${{ needs.automated-tests.result }}"
          RAG_TESTS="${{ needs.rag-integration.result }}"
          SECURITY="${{ needs.security-audit.result }}"
          PERFORMANCE="${{ needs.performance-benchmark.result }}"
          
          echo "## Validation Results" >> validation-summary.md
          echo "" >> validation-summary.md
          
          # Function to get status emoji
          get_status() {
            case "$1" in
              "success") echo "✅" ;;
              "failure") echo "❌" ;;
              "cancelled") echo "⏹️" ;;
              "skipped") echo "⏭️" ;;
              *) echo "❔" ;;
            esac
          }
          
          echo "- $(get_status "$CODE_QUALITY") Code Quality: $CODE_QUALITY" >> validation-summary.md
          echo "- $(get_status "$TESTS") Automated Tests: $TESTS" >> validation-summary.md
          echo "- $(get_status "$RAG_TESTS") RAG Integration: $RAG_TESTS" >> validation-summary.md
          echo "- $(get_status "$SECURITY") Security Audit: $SECURITY" >> validation-summary.md
          echo "- $(get_status "$PERFORMANCE") Performance: $PERFORMANCE" >> validation-summary.md
          echo "" >> validation-summary.md
          
          # Overall status
          OVERALL_SUCCESS=true
          for status in "$CODE_QUALITY" "$TESTS" "$RAG_TESTS" "$SECURITY" "$PERFORMANCE"; do
            if [ "$status" = "failure" ]; then
              OVERALL_SUCCESS=false
              break
            fi
          done
          
          if [ "$OVERALL_SUCCESS" = true ]; then
            echo "## ✅ Overall Status: PASSED" >> validation-summary.md
            echo "" >> validation-summary.md
            echo "All validations passed! This contribution is ready for review." >> validation-summary.md
          else
            echo "## ❌ Overall Status: FAILED" >> validation-summary.md
            echo "" >> validation-summary.md
            echo "Some validations failed. Please review the results and fix any issues." >> validation-summary.md
          fi
          
          echo "" >> validation-summary.md
          echo "## Next Steps" >> validation-summary.md
          echo "" >> validation-summary.md
          
          if [ "$OVERALL_SUCCESS" = true ]; then
            echo "1. 👀 Code review by maintainers" >> validation-summary.md
            echo "2. 📝 Address any review feedback" >> validation-summary.md
            echo "3. 🎉 Merge when approved" >> validation-summary.md
          else
            echo "1. 🔧 Fix failing validations" >> validation-summary.md
            echo "2. ♾️ Push fixes to trigger re-validation" >> validation-summary.md
            echo "3. 🔁 Repeat until all checks pass" >> validation-summary.md
          fi
          
          echo "" >> validation-summary.md
          echo "---" >> validation-summary.md
          echo "*Automated validation by Claudette Community CI*" >> validation-summary.md
          
          cat validation-summary.md

      - name: Upload validation summary
        uses: actions/upload-artifact@v4
        with:
          name: validation-summary
          path: validation-summary.md
          retention-days: 90

      - name: Comment on PR (if applicable)
        if: github.event_name == 'pull_request'
        run: |
          echo "This would post a comment on the PR with validation results"
          # In a real implementation, this would use gh api to post a comment
          # with the validation summary

  # Cleanup
  cleanup:
    name: 🧹 Cleanup
    runs-on: ubuntu-latest
    needs: [validation-summary]
    if: always()
    steps:
      - name: Cleanup temporary resources
        run: |
          echo "Cleaning up temporary resources..."
          # This would clean up any temporary resources
          # created during the validation process
          echo "Cleanup completed"
