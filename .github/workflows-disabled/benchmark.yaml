name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run benchmarks daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    # Allow manual trigger

jobs:
  benchmark:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.11', '3.12']
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/pyproject.toml') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
    
    - name: Download baseline benchmark
      id: download-baseline
      continue-on-error: true
      uses: actions/download-artifact@v3
      with:
        name: benchmark-baseline-py${{ matrix.python-version }}
        path: .
    
    - name: Run benchmarks
      env:
        # Mock API keys for testing (not real keys)
        OPENAI_API_KEY: "sk-test-key-for-benchmark-testing-only"
        ANTHROPIC_API_KEY: "sk-ant-test-key-for-benchmark-testing-only"
      run: |
        echo "ðŸš€ Running performance benchmarks..."
        python benchmark_runner.py --output current_benchmark.json
    
    - name: Compare with baseline
      if: steps.download-baseline.outcome == 'success'
      run: |
        echo "ðŸ“Š Comparing with baseline..."
        python benchmark_runner.py --compare current_benchmark.json
    
    - name: Save new baseline (on main branch)
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      run: |
        echo "ðŸ’¾ Saving new baseline..."
        cp current_benchmark.json baseline-py${{ matrix.python-version }}.json
    
    - name: Upload baseline artifact
      if: github.ref == 'refs/heads/main' && github.event_name == 'push'
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-baseline-py${{ matrix.python-version }}
        path: baseline-py${{ matrix.python-version }}.json
        retention-days: 90
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results-py${{ matrix.python-version }}-${{ github.run_number }}
        path: current_benchmark.json
        retention-days: 30
    
    - name: Performance regression check
      run: |
        echo "ðŸ” Checking for performance regressions..."
        
        # Check if comparison failed (exit code 1)
        if [ -f baseline-py${{ matrix.python-version }}.json ]; then
          python benchmark_runner.py --compare current_benchmark.json || {
            echo "âŒ Performance regression detected!"
            echo "Benchmark comparison failed the performance thresholds:"
            echo "â€¢ Latency increase > 20% = FAIL"
            echo "â€¢ Memory increase > 15% = FAIL"
            exit 1
          }
        else
          echo "âš ï¸ No baseline found for comparison"
        fi
    
    - name: Generate performance report
      if: always()
      run: |
        echo "ðŸ“ Generating performance report..."
        
        cat > performance_report.md << 'EOF'
        # Performance Benchmark Report
        
        **Python Version:** ${{ matrix.python-version }}  
        **Run Date:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')  
        **Commit:** ${{ github.sha }}  
        **Branch:** ${{ github.ref_name }}  
        
        ## Benchmark Results
        
        EOF
        
        if [ -f current_benchmark.json ]; then
          echo "âœ… Benchmarks completed successfully" >> performance_report.md
          
          # Extract key metrics from JSON (basic parsing)
          if command -v jq >/dev/null 2>&1; then
            echo "" >> performance_report.md
            echo "### Key Metrics" >> performance_report.md
            echo "" >> performance_report.md
            
            # Try to extract metrics if jq is available
            total_tests=$(jq -r '.benchmarks | length // 0' current_benchmark.json 2>/dev/null || echo "N/A")
            echo "- **Total Tests:** $total_tests" >> performance_report.md
            
            # Add performance thresholds
            echo "" >> performance_report.md
            echo "### Performance Thresholds" >> performance_report.md
            echo "- Compression latency: < 300ms average" >> performance_report.md
            echo "- Backend switch overhead: < 40ms" >> performance_report.md
            echo "- Cache hit round-trip: < 150ms" >> performance_report.md
            echo "- Memory peak under load: < 150MB RSS" >> performance_report.md
          fi
        else
          echo "âŒ Benchmark execution failed" >> performance_report.md
        fi
        
        # Add comparison results if available
        if [ -f baseline-py${{ matrix.python-version }}.json ]; then
          echo "" >> performance_report.md
          echo "### Baseline Comparison" >> performance_report.md
          echo "Comparison with previous baseline completed." >> performance_report.md
        else
          echo "" >> performance_report.md
          echo "### Baseline Comparison" >> performance_report.md
          echo "No baseline available for comparison." >> performance_report.md
        fi
    
    - name: Upload performance report
      uses: actions/upload-artifact@v3
      with:
        name: performance-report-py${{ matrix.python-version }}
        path: performance_report.md
        retention-days: 30
    
    - name: Comment on PR (if PR)
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          let reportContent = '## ðŸ“Š Performance Benchmark Results\n\n';
          reportContent += `**Python ${{ matrix.python-version }}**\n\n`;
          
          try {
            if (fs.existsSync('performance_report.md')) {
              const report = fs.readFileSync('performance_report.md', 'utf8');
              reportContent += report;
            } else {
              reportContent += 'âŒ Performance report not generated\n';
            }
          } catch (error) {
            reportContent += `âŒ Error reading performance report: ${error.message}\n`;
          }
          
          reportContent += '\n---\n';
          reportContent += 'ðŸ”— [View detailed results in Actions](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})\n';
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: reportContent
          });

  update-docs:
    runs-on: ubuntu-latest
    needs: benchmark
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Download benchmark results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results-py3.11-${{ github.run_number }}
        path: .
    
    - name: Update documentation
      run: |
        echo "ðŸ“ Updating benchmark documentation..."
        
        # Create docs/results directory if it doesn't exist
        mkdir -p docs/results
        
        # Generate latest benchmark report
        cat > docs/results/latest_benchmark.md << 'EOF'
        # Latest Benchmark Results
        
        **Last Updated:** $(date -u +'%Y-%m-%d %H:%M:%S UTC')  
        **Commit:** ${{ github.sha }}  
        
        ## Performance Metrics
        
        | Metric | Target | Latest Result | Status |
        |--------|--------|---------------|---------|
        | Compression Latency | < 300ms | TBD | âœ… |
        | Backend Switch | < 40ms | TBD | âœ… |
        | Cache Hit | < 150ms | TBD | âœ… |
        | Memory Peak | < 150MB | TBD | âœ… |
        
        ## Benchmark Details
        
        Results are automatically updated on each commit to main branch.
        
        - **Test Environment:** Ubuntu Latest, Python 3.11
        - **Test Framework:** pytest-benchmark
        - **Monitoring:** Automated via GitHub Actions
        
        ## Performance Thresholds
        
        - âŒ **FAIL:** Latency increase > 20% or Memory increase > 15%
        - âš ï¸ **WARN:** Latency increase > 10% or Memory increase > 10%
        - âœ… **PASS:** Within acceptable performance bounds
        
        EOF
        
        # Copy benchmark data for documentation
        if [ -f current_benchmark.json ]; then
          cp current_benchmark.json docs/results/latest_benchmark_data.json
        fi
    
    - name: Commit documentation updates
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        git add docs/results/
        
        if git diff --staged --quiet; then
          echo "No documentation changes to commit"
        else
          git commit -m "ðŸ“Š Update benchmark documentation [skip ci]"
          git push
        fi